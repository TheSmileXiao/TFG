{"cells":[{"cell_type":"markdown","metadata":{"id":"Q6FgSf7FVR6Q"},"source":["# Clasificaciones de imagenes con YOLO"]},{"cell_type":"markdown","metadata":{"id":"1dXDRhinVLJz"},"source":["##Importar los paquetes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Uka5FzRZHAx"},"outputs":[],"source":["!pip install ultralytics fiftyone"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23319,"status":"ok","timestamp":1685473338328,"user":{"displayName":"Êù®Êôì","userId":"10017042992468394033"},"user_tz":-120},"id":"b3709f28","outputId":"5dd389ef-316c-47dc-9e4c-cf197bd81036"},"outputs":[{"name":"stdout","output_type":"stream","text":["Migrating database to v0.20.1\n"]},{"name":"stderr","output_type":"stream","text":["INFO:fiftyone.migrations.runner:Migrating database to v0.20.1\n"]}],"source":["import zipfile\n","import os\n","import shutil\n","import random\n","import fiftyone               as fo\n","import fiftyone.utils.random        as four\n","import matplotlib.pyplot          as plt\n","from ultralytics          import YOLO\n","from google.colab         import drive"]},{"cell_type":"markdown","metadata":{"id":"KEDoq_6wscqp"},"source":["##BUILD ROBOCUP_2019 Dataset"]},{"cell_type":"markdown","metadata":{"id":"rcq1RcLAVFKo"},"source":["###Dataset details\n","RoboCup Dataset contains 196195 images.\n","\n","8 main parent categories that contain 180 children categories.\n","\n","70% for train\n","\n","20% for valid\n","\n","10% for test"]},{"cell_type":"markdown","metadata":{"id":"3t30zfuDvGrk"},"source":["###Build dataset"]},{"cell_type":"markdown","metadata":{"id":"H5zX8cyONJu0"},"source":["###Connect to my oneDrive"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":77556,"status":"ok","timestamp":1685473415880,"user":{"displayName":"Êù®Êôì","userId":"10017042992468394033"},"user_tz":-120},"id":"llE6wo9eZMd2","outputId":"750660f4-df4f-48a9-d523-ab618f6e8fa9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["drive.mount('/content/drive')\n","ruta_archivo_zip = '/content/drive/MyDrive/Colab Notebooks/data_subclass.zip'\n","ruta_destino = './'\n","with zipfile.ZipFile(ruta_archivo_zip, 'r') as archivo_zip:\n","    # Extraer el archivo deseado en la ruta de destino\n","    archivo_zip.extractall(ruta_destino)"]},{"cell_type":"markdown","metadata":{"id":"QYkS_rmENLm3"},"source":["###transforming data to suit the yolo model"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1685473415881,"user":{"displayName":"Êù®Êôì","userId":"10017042992468394033"},"user_tz":-120},"id":"WHTM173BtI-i"},"outputs":[],"source":["def transform_dataset(root_dir, output_dir, train_ratio=0.8):\n","    # Create the output directory\n","    os.makedirs(output_dir, exist_ok=True)\n","    train_dir = os.path.join(output_dir, 'train')\n","    val_dir = os.path.join(output_dir, 'val')\n","    os.makedirs(train_dir, exist_ok=True)\n","    os.makedirs(val_dir, exist_ok=True)\n","    \n","    # Get the list of classes in the root directory\n","    classes = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n","    \n","    for class_name in classes:\n","        class_dir = os.path.join(root_dir, class_name)\n","        if not os.path.isdir(class_dir):\n","            continue\n","        \n","        # Get the list of images in the current class\n","        images = [f for f in os.listdir(class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n","        \n","        # Shuffle the images randomly\n","        random.shuffle(images)\n","        \n","        # Calculate the number of training images based on the specified ratio\n","        train_size = int(len(images) * train_ratio)\n","        \n","        # Create the class directories in the training and validation sets\n","        train_class_dir = os.path.join(train_dir, class_name)\n","        val_class_dir = os.path.join(val_dir, class_name)\n","        os.makedirs(train_class_dir, exist_ok=True)\n","        os.makedirs(val_class_dir, exist_ok=True)\n","        \n","        # Copy the training images\n","        for img in images[:train_size]:\n","            src_path = os.path.join(class_dir, img)\n","            dst_path = os.path.join(train_class_dir, f\"{os.path.splitext(img)[0]}_{class_name}.png\")\n","            shutil.copy(src_path, dst_path)\n","        \n","        # Copy the validation images\n","        for img in images[train_size:]:\n","            src_path = os.path.join(class_dir, img)\n","            dst_path = os.path.join(val_class_dir, f\"{os.path.splitext(img)[0]}_{class_name}.png\")\n","            shutil.copy(src_path, dst_path)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":52837,"status":"ok","timestamp":1685473468706,"user":{"displayName":"Êù®Êôì","userId":"10017042992468394033"},"user_tz":-120},"id":"sBm_gNgiwU9c"},"outputs":[],"source":["root_dir   = '/content/training_data'   # Path to the original dataset\n","output_dir  = '/content/dataset'      # Path for the resized dataset\n","\n","transform_dataset(root_dir, output_dir, train_ratio=0.8)"]},{"cell_type":"markdown","metadata":{"id":"kXcB15IqNqKl"},"source":["##YOLO"]},{"cell_type":"markdown","metadata":{"id":"xwTPMtNSNvni"},"source":["###Train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"mPNa-Gk5b_1-"},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                 \n","  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]               \n","  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]           \n","  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]              \n","  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]           \n","  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]              \n","  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]           \n","  7                  -1  1   7375360  ultralytics.nn.modules.conv.Conv             [640, 1280, 3, 2]             \n","  8                  -1  3  27865600  ultralytics.nn.modules.block.C2f             [1280, 1280, 3, True]         \n","  9                  -1  1   2921960  ultralytics.nn.modules.head.Classify         [1280, 1000]                  \n","YOLOv8x-cls summary: 183 layers, 57422840 parameters, 57422840 gradients\n","Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-cls.pt to yolov8x-cls.pt...\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 110M/110M [00:00\u003c00:00, 144MB/s] \n","Transferred 302/302 items from pretrained weights\n","Ultralytics YOLOv8.0.111 üöÄ Python-3.10.11 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8x-cls.yaml, data=/content/dataset, epochs=100, patience=50, batch=16, imgsz=224, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/classify/train\n","Overriding model.yaml nc=1000 with nc=180\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                 \n","  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]               \n","  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]           \n","  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]              \n","  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]           \n","  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]              \n","  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]           \n","  7                  -1  1   7375360  ultralytics.nn.modules.conv.Conv             [640, 1280, 3, 2]             \n","  8                  -1  3  27865600  ultralytics.nn.modules.block.C2f             [1280, 1280, 3, True]         \n","  9                  -1  1   1871540  ultralytics.nn.modules.head.Classify         [1280, 180]                   \n","YOLOv8x-cls summary: 183 layers, 56372420 parameters, 56372420 gradients\n","Transferred 300/302 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train', view at http://localhost:6006/\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n","Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to yolov8n.pt...\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.23M/6.23M [00:00\u003c00:00, 84.1MB/s]\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 50 weight(decay=0.0), 51 weight(decay=0.0005), 51 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mRandomResizedCrop(p=1.0, height=224, width=224, scale=(0.5, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=1), HorizontalFlip(p=0.5), ColorJitter(p=0.5, brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.30000000000000004, 1.7], hue=[-0.015, 0.015]), Normalize(p=1.0, mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0), max_pixel_value=255.0), ToTensorV2(always_apply=True, p=1.0, transpose_mask=False)\n","Image sizes 224 train, 224 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/classify/train\u001b[0m\n","Starting training for 100 epochs...\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n","      1/100      2.02G      1.303         16        224:   0%|          | 3/9806 [00:02\u003c1:25:04,  1.92it/s]Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n","      1/100      2.02G      1.303         16        224:   0%|          | 3/9806 [00:02\u003c1:25:04,  1.92it/s]Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n","      1/100      2.02G      1.303         16        224:   0%|          | 5/9806 [00:02\u003c50:54,  3.21it/s]  Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n","\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 755k/755k [00:00\u003c00:00, 12.8MB/s]\n","      1/100      2.02G      1.302         16        224:   0%|          | 6/9806 [00:02\u003c49:08,  3.32it/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 755k/755k [00:00\u003c00:00, 15.5MB/s]\n","\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 755k/755k [00:00\u003c00:00, 16.5MB/s]\n","      1/100      2.06G      1.211          3        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9806/9806 [22:06\u003c00:00,  7.39it/s]\n","               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1229/1229 [01:45\u003c00:00, 11.67it/s]\n","                   all     0.0917      0.283\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n","      2/100      2.15G      1.015          3        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9806/9806 [20:15\u003c00:00,  8.07it/s]\n","               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1229/1229 [01:45\u003c00:00, 11.68it/s]\n","                   all      0.205      0.487\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n","      3/100      2.17G     0.9192         16        224:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 6609/9806 [13:19\u003c06:07,  8.71it/s]"]}],"source":["# Load a model\n","model = YOLO('yolov8x-cls.yaml').load('yolov8x-cls.pt')  # build from YAML and transfer weights\n","\n","# Train the model\n","model.train(data='/content/dataset', epochs=100, imgsz=224, verbose=True)"]},{"cell_type":"markdown","metadata":{"id":"UKvHXG9iONfw"},"source":["###Val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"alfdUzcqOO6j"},"outputs":[],"source":["# Validate the model\n","metrics = model.val()  # no arguments needed, dataset and settings remembered\n","metrics.top1   # top1 accuracy\n","metrics.top5   # top5 accuracy"]},{"cell_type":"markdown","metadata":{"id":"jOGHteVXOvd2"},"source":["###Export"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b-PZJ8qFOw06"},"outputs":[],"source":["# Export the model\n","model.export(format='onnx')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7WdJqVMSwndK"},"outputs":[],"source":["print(metrics.top1)\n","print(metrics.top5)"]}],"metadata":{"accelerator":"GPU","colab":{"name":"","toc_visible":true,"version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}