{"cells":[{"cell_type":"markdown","metadata":{"id":"riskaa-IcEbL"},"source":["# Hierarchically Deep Convolutional Neural Network For Image Recognition"]},{"cell_type":"markdown","metadata":{"id":"n0K3tjNocEbP"},"source":["## Setup and Imports"]},{"cell_type":"markdown","metadata":{"id":"l-So1fsTcEbP"},"source":["**Import Packages**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9UP5TDk2cEbP"},"outputs":[],"source":["import keras as kr\n","import numpy as np\n","import tensorflow as tf\n","\n","from keras.datasets import cifar100\n","\n","from sklearn.model_selection import train_test_split\n","\n","from random import randint\n","import time\n","import os\n","import shutil\n","import zipfile\n","import matplotlib.pyplot as plt\n","import csv\n","from PIL import Image\n","from sklearn.preprocessing import LabelEncoder"]},{"cell_type":"markdown","metadata":{"id":"a68vfT-_cEbR"},"source":["**Define Global Variables**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o_VHWJvFcEbS"},"outputs":[],"source":["# The number of coarse categories\n","coarse_categories = 8\n","\n","# The number of fine categories\n","fine_categories = 180"]},{"cell_type":"markdown","metadata":{"id":"KEDoq_6wscqp"},"source":["##BUILD ROBOCUP_2019 Dataset"]},{"cell_type":"markdown","metadata":{"id":"rcq1RcLAVFKo"},"source":["###Dataset details\n","RoboCup Dataset contains 196195 images.\n","\n","8 main parent categories that contain 180 children categories.\n","\n","70% for train\n","\n","30% for test\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3145,"status":"ok","timestamp":1682647202822,"user":{"displayName":"杨晓","userId":"10498861279155438245"},"user_tz":-120},"id":"1NBsXoPnCQOE","outputId":"89178c7f-e18d-4301-9141-b44cf41c4195"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sdo18pR2t_lc"},"outputs":[],"source":["ruta_archivo_zip = './drive/MyDrive/dataset.zip'\n","ruta_destino = './'\n","with zipfile.ZipFile(ruta_archivo_zip, 'r') as archivo_zip:\n","    # Extraer el archivo deseado en la ruta de destino\n","    archivo_zip.extractall(ruta_destino)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yW9KzHUPzqS4"},"outputs":[],"source":["def load_data(image_size):\n","  images = []\n","  labels_c = []\n","  labels = []\n","\n","  with open('./data/train.csv', 'r') as f:\n","    reader = csv.reader(f)\n","    data = list(reader)\n","    for item in data:\n","      img = Image.open(data[0][0].replace('\\\\','/'))\n","      img = img.resize((32,32))\n","      img = np.array(img)\n","      images.append(img)\n","      labels_c.append(item[1])\n","      labels.append(item[2])\n","    x_train = np.array(images, dtype=np.uint8)\n","    y_train_c = np.array(labels_c)\n","    y_train = np.array(labels)\n","    encoder = LabelEncoder()\n","    y_train_c = encoder.fit_transform(y_train_c) #convertir en dtype uint8\n","    y_train = encoder.fit_transform(y_train)\n","    y_train_c = y_train_c.reshape((-1,1))    #shape = (137337,) to (137337,1)\n","    y_train = y_train.reshape((-1,1))\n","    print(\"x_train shape:\", x_train.shape)\n","    print(\"y_train_c shape:\", y_train_c.shape)\n","    print(\"y_train shape:\", y_train.shape)\n","\n","  images.clear()\n","  labels_c.clear()\n","  labels.clear()\n","\n","  with open('./data/test.csv', 'r') as f:\n","    reader = csv.reader(f)\n","    data = list(reader)\n","    for item in data:\n","      img = Image.open(data[0][0].replace('\\\\','/'))\n","      img = img.resize((32,32))\n","      img = np.array(img)\n","      images.append(img)\n","      labels_c.append(item[1])\n","      labels.append(item[2])\n","    x_test = np.array(images, dtype=np.uint8)\n","    y_test_c = np.array(labels_c)\n","    y_test = np.array(labels)\n","    encoder = LabelEncoder()\n","    y_test_c = encoder.fit_transform(y_test_c) #convertir en dtype uint8\n","    y_test = encoder.fit_transform(y_test)\n","    y_test_c = y_test_c.reshape((-1,1))    #shape = (58859,) to (58859,1)\n","    y_test = y_test.reshape((-1,1))\n","    print(\"x_test shape:\", x_test.shape)\n","    print(\"y_test_c shape:\", y_test_c.shape)\n","    print(\"y_test shape:\", y_test.shape)\n","\n","  return (x_train, y_train_c, y_train),(x_test, y_test_c, y_test)"]},{"cell_type":"markdown","metadata":{"id":"NhVvB1XfXrNc"},"source":["**Load RoboCup Data Set**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":187058,"status":"ok","timestamp":1682647437250,"user":{"displayName":"杨晓","userId":"10498861279155438245"},"user_tz":-120},"id":"wjQyDb_1X1tj","outputId":"2f1ece13-128a-49f9-884b-41e43ca6a4dc"},"outputs":[{"name":"stdout","output_type":"stream","text":["x_train shape: (137337, 32, 32, 3)\n","y_train_c shape: (137337, 1)\n","y_train shape: (137337, 1)\n","x_test shape: (58859, 32, 32, 3)\n","y_test_c shape: (58859, 1)\n","y_test shape: (58859, 1)\n"]}],"source":["(X, y_c, y), (x_test, y_c_test, y_test) = load_data(image_size=32)"]},{"cell_type":"markdown","metadata":{"id":"5IioU2NgcEbS"},"source":["##Preprocess Dataset"]},{"cell_type":"markdown","metadata":{"id":"98WhmsYXcEbS"},"source":["**Import Cifar100 Data Set**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pG5KOEkPcEbT"},"outputs":[],"source":["#(X, y_c), (x_test, y_c_test) = cifar100.load_data(label_mode='coarse')\n","#(X, y), (x_test, y_test) = cifar100.load_data(label_mode='fine')"]},{"cell_type":"markdown","metadata":{"id":"nm_QuOYRcEbT"},"source":["**Fine-To-Coarse Mapping**\n","\n","(Ideally, this would be done through spectral clustering as opposed to hard-coding)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y0EElnRacEbT"},"outputs":[],"source":["fine2coarse = np.zeros((fine_categories,coarse_categories))\n","for i in range(coarse_categories):\n","    index = np.where(y_c_test[:,0] == i)[0]\n","    fine_cat = np.unique([y_test[j,0] for j in index])\n","    for j in fine_cat:\n","        fine2coarse[j,i] = 1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vKQYzQkocEbT"},"outputs":[],"source":["y_c = 0; # Clear y_c in interest of saving mem\n","y_c_test=0;"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oMlrc0kscEbT"},"outputs":[],"source":["################################################################################\n","#    Title: One Hot Encoding\n","################################################################################\n","#    Description:\n","#        This function extends a matrix to one-hot encoding\n","#\n","#    Parameters:\n","#        y    Array of label values\n","#\n","#    Returns:\n","#        y_new    One hot encoded array of labels\n","################################################################################\n","def one_hot(y):\n","    n_values = np.max(y) + 1\n","    y_new = np.eye(n_values)[y[:,0]]\n","    return y_new"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1682647437252,"user":{"displayName":"杨晓","userId":"10498861279155438245"},"user_tz":-120},"id":"fsvffDyCcEbU","outputId":"9b0b33e2-0f7e-403c-a88d-534917005b04"},"outputs":[{"name":"stdout","output_type":"stream","text":["(137337, 180)\n"]}],"source":["y=one_hot(y)\n","y_test=one_hot(y_test)\n","print(np.shape(y))"]},{"cell_type":"markdown","metadata":{"id":"w5ijumSIcEbU"},"source":["**Apply ZCA Whitening**\n","\n","La transformación ZCA Whitening se utiliza principalmente para reducir la correlación entre los píxeles de las imágenes y mejorar la capacidad de los algoritmos de aprendizaje para extraer características útiles de las mismas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DNw1zvTWcEbU"},"outputs":[],"source":["################################################################################\n","#    Title: ZCA\n","################################################################################\n","#    Description:\n","#        This function applies ZCA Whitening to the image set\n","#\n","#    Parameters:\n","#        x_1           Array of MxNxC images to compute the ZCA Whitening\n","#        x_2           Array of MxNxC images to apply the ZCA transform\n","#        num_batch    Number of batches to do the computation\n","#\n","#    Returns:\n","#        An array of MxNxC zca whitened images\n","################################################################################\n","def zca(x_1, x_2, epsilon=1e-5):\n","\n","    with tf.name_scope('ZCA'):\n","        tf.compat.v1.disable_eager_execution()\n","\n","        x1 = tf.compat.v1.placeholder(tf.float64, shape=np.shape(x_1), name='placeholder_x1')\n","        x2 = tf.compat.v1.placeholder(tf.float64, shape=np.shape(x_2), name='placeholder_x2')\n","\n","        flatx = tf.cast(tf.reshape(x1, (-1, np.prod(x_1.shape[-3:])),name=\"reshape_flat\"),tf.float64,name=\"flatx\")\n","        sigma = tf.tensordot(tf.transpose(flatx),flatx, 1,name=\"sigma\") / tf.cast(tf.shape(flatx)[0],tf.float64) ### N-1 or N?\n","        s, u, v = tf.compat.v1.svd(sigma,name=\"svd\")\n","        pc = tf.tensordot(tf.tensordot(u,tf.compat.v1.diag(1. / tf.sqrt(s+epsilon)),1,name=\"inner_dot\"),tf.transpose(u),1, name=\"pc\")\n","\n","        net1 = tf.tensordot(flatx, pc,1,name=\"whiten1\")\n","        net1 = tf.reshape(net1,np.shape(x_1), name=\"output1\")\n","\n","        flatx2 = tf.cast(tf.reshape(x2, (-1, np.prod(x_2.shape[-3:])),name=\"reshape_flat2\"),tf.float64,name=\"flatx2\")\n","        net2 = tf.tensordot(flatx2, pc,1,name=\"whiten2\")\n","        net2 = tf.reshape(net2,np.shape(x_2), name=\"output2\")\n","\n","    with tf.compat.v1.Session() as sess:\n","            sess.run(tf.compat.v1.global_variables_initializer())\n","            x_1,x_2 = sess.run([net1,net2], feed_dict={x1: x_1, x2: x_2})\n","    return x_1,x_2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19980,"status":"ok","timestamp":1682647457218,"user":{"displayName":"杨晓","userId":"10498861279155438245"},"user_tz":-120},"id":"HVEBDcLNcEbV","outputId":"4a14360a-2d3a-4c54-f858-ce1858f0d075"},"outputs":[{"name":"stdout","output_type":"stream","text":["Time Elapsed - ZCA Whitening: 19.791391611099243\n"]}],"source":["time1 = time.time()\n","X,x_test = zca(X,x_test)\n","time2 = time.time()\n","print('Time Elapsed - ZCA Whitening: '+str(time2-time1));"]},{"cell_type":"markdown","metadata":{"id":"kxQHfeDfcEbV"},"source":["**Split Training set into Training and Validation sets**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5W4EZ8H1cEbV"},"outputs":[],"source":["x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=.1, random_state=0)\n","X = 0\n","y = 0"]},{"cell_type":"markdown","metadata":{"id":"3h3URSXVcEbV"},"source":["**Flip, pad and randomly crop each photo**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mD26EUVscEbV"},"outputs":[],"source":["################################################################################\n","#    Title: Preprocess Img\n","################################################################################\n","#    Description:\n","#        This function pads images by 4 pixels, randomly crops them, then\n","#        randomly flips them\n","#\n","#    Parameters:\n","#        x_1           Array of MxNxC images to compute the ZCA Whitening\n","#        x_2           Array of MxNxC images to apply the ZCA transform\n","#        num_batch    Number of batches to do the computation\n","#\n","#    Returns:\n","#        An array of MxNxC zca whitened images\n","################################################################################\n","def preprocess_img(X,y):\n","\n","    with tf.name_scope('Preproc'):\n","        tf.compat.v1.disable_eager_execution()\n","\n","        images = tf.compat.v1.placeholder(tf.float64, shape=np.shape(X))\n","        labels = tf.compat.v1.placeholder(tf.float64, shape=np.shape(y))\n","\n","        net = tf.map_fn(lambda img: tf.image.flip_left_right(img), images)\n","        net = tf.map_fn(lambda img: tf.image.rot90(img), net)\n","        net = tf.compat.v1.image.resize_image_with_crop_or_pad(net,40,40)\n","        net = tf.map_fn(lambda img: tf.compat.v1.random_crop(img, [32,32,3]), net)\n","\n","        net1 = tf.compat.v1.image.resize_image_with_crop_or_pad(images,40,40)\n","        net1 = tf.map_fn(lambda img: tf.compat.v1.random_crop(img, [32,32,3]), net1)\n","\n","        net = tf.concat([net, net1],0)\n","        net = tf.compat.v1.random_shuffle(net, seed=0)\n","        net_labels = tf.concat([labels, labels],0)\n","        net_labels = tf.compat.v1.random_shuffle(net_labels,seed=0)\n","\n","        net = tf.map_fn(lambda img: tf.image.random_flip_up_down(img), net)\n","\n","    with tf.compat.v1.Session() as sess:\n","            sess.run(tf.compat.v1.global_variables_initializer())\n","            x_t,y_t = sess.run([net,net_labels], feed_dict={images: X, labels: y})\n","    return x_t,y_t\n"]},{"cell_type":"markdown","metadata":{"id":"PRuL3D9CcEbW"},"source":["## Single Classifier Training"]},{"cell_type":"markdown","metadata":{"id":"V3HqI_ZrcEbW"},"source":["**Constructing CNN**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aKPzubeQcEbW"},"outputs":[],"source":["from keras import optimizers\n","from keras.layers import Input, Conv2D, Dropout, MaxPooling2D, Flatten, Dense\n","from keras.models import Model\n","\n","in_layer = Input(shape=(32, 32, 3), dtype='float32', name='main_input')\n","\n","net = Conv2D(384, 3, strides=1, padding='same', activation='elu')(in_layer)\n","net = MaxPooling2D((2, 2), padding='valid')(net)\n","\n","net = Conv2D(384, 1, strides=1, padding='same', activation='elu')(net)\n","net = Conv2D(384, 2, strides=1, padding='same', activation='elu')(net)\n","net = Conv2D(640, 2, strides=1, padding='same', activation='elu')(net)\n","net = Conv2D(640, 2, strides=1, padding='same', activation='elu')(net)\n","net = Dropout(.2)(net)\n","net = MaxPooling2D((2, 2), padding='valid')(net)\n","\n","net = Conv2D(640, 1, strides=1, padding='same', activation='elu')(net)\n","net = Conv2D(768, 2, strides=1, padding='same', activation='elu')(net)\n","net = Conv2D(768, 2, strides=1, padding='same', activation='elu')(net)\n","net = Conv2D(768, 2, strides=1, padding='same', activation='elu')(net)\n","net = Dropout(.3)(net)\n","net = MaxPooling2D((2, 2), padding='valid')(net)\n","\n","net = Conv2D(768, 1, strides=1, padding='same', activation='elu')(net)\n","net = Conv2D(896, 2, strides=1, padding='same', activation='elu')(net)\n","net = Conv2D(896, 2, strides=1, padding='same', activation='elu')(net)\n","net = Dropout(.4)(net)\n","net = MaxPooling2D((2, 2), padding='valid')(net)\n","\n","net = Conv2D(896, 3, strides=1, padding='same', activation='elu')(net)\n","net = Conv2D(1024, 2, strides=1, padding='same', activation='elu')(net)\n","net = Conv2D(1024, 2, strides=1, padding='same', activation='elu')(net)\n","net = Dropout(.5)(net)\n","net = MaxPooling2D((2, 2), padding='valid')(net)\n","\n","net = Conv2D(1024, 1, strides=1, padding='same', activation='elu')(net)\n","net = Conv2D(1152, 2, strides=1, padding='same', activation='elu')(net)\n","net = Dropout(.6)(net)\n","net = MaxPooling2D((2, 2), padding='same')(net)\n","\n","net = Flatten()(net)\n","net = Dense(1152, activation='elu')(net)\n","#net = Dense(100, activation='softmax')(net)\n","net = Dense(180, activation='softmax')(net)"]},{"cell_type":"markdown","metadata":{"id":"xi1yvk-BcEbX"},"source":["**Compile Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YQjmvrLAcEbX"},"outputs":[],"source":["model = Model(inputs=in_layer,outputs=net)\n","sgd_coarse = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n","model.compile(optimizer= sgd_coarse, loss='categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_DJKGufHcEbX"},"outputs":[],"source":["#model.load_weights('data/models/model_coarse'+str(30))"]},{"cell_type":"markdown","metadata":{"id":"7PwY_kKncEbX"},"source":["**Train Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zA7lgBUHcEbX"},"outputs":[],"source":["tbCallBack = kr.callbacks.TensorBoard(log_dir='./data/graph/elu_drop/', histogram_freq=0, write_graph=True, write_images=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NJo-PYW0cEbX"},"outputs":[],"source":["batch = 64"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"lt5CRYzCcEbX","outputId":"baa68c41-fd15-490a-a698-361d19557904"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train on 123603 samples, validate on 13734 samples\n","Epoch 1/5\n","123603/123603 [==============================] - ETA: 0s - loss: 5.1889 - accuracy: 0.0065"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates = self.state_updates\n"]},{"name":"stdout","output_type":"stream","text":["123603/123603 [==============================] - 181s 1ms/sample - loss: 5.1889 - accuracy: 0.0065 - val_loss: 5.1862 - val_accuracy: 0.0068\n","Epoch 2/5\n","123603/123603 [==============================] - 172s 1ms/sample - loss: 5.1876 - accuracy: 0.0069 - val_loss: 5.1872 - val_accuracy: 0.0077\n","Epoch 3/5\n","123603/123603 [==============================] - 172s 1ms/sample - loss: 5.1878 - accuracy: 0.0066 - val_loss: 5.1864 - val_accuracy: 0.0068\n","Epoch 4/5\n","123603/123603 [==============================] - 172s 1ms/sample - loss: 5.1876 - accuracy: 0.0061 - val_loss: 5.1865 - val_accuracy: 0.0059\n","Epoch 5/5\n","123603/123603 [==============================] - 172s 1ms/sample - loss: 5.1874 - accuracy: 0.0063 - val_loss: 5.1869 - val_accuracy: 0.0067\n","Train on 123603 samples, validate on 13734 samples\n","Epoch 6/10\n","123603/123603 [==============================] - 172s 1ms/sample - loss: 5.1874 - accuracy: 0.0062 - val_loss: 5.1879 - val_accuracy: 0.0077\n","Epoch 7/10\n","123603/123603 [==============================] - 172s 1ms/sample - loss: 5.1875 - accuracy: 0.0061 - val_loss: 5.1868 - val_accuracy: 0.0077\n","Epoch 8/10\n","123603/123603 [==============================] - 172s 1ms/sample - loss: 5.1872 - accuracy: 0.0064 - val_loss: 5.1876 - val_accuracy: 0.0077\n","Epoch 9/10\n","123603/123603 [==============================] - 172s 1ms/sample - loss: 5.1872 - accuracy: 0.0064 - val_loss: 5.1877 - val_accuracy: 0.0077\n","Epoch 10/10\n","123603/123603 [==============================] - 171s 1ms/sample - loss: 5.1873 - accuracy: 0.0062 - val_loss: 5.1871 - val_accuracy: 0.0067\n","Train on 123603 samples, validate on 13734 samples\n","Epoch 11/15\n","123603/123603 [==============================] - 171s 1ms/sample - loss: 5.1872 - accuracy: 0.0062 - val_loss: 5.1868 - val_accuracy: 0.0067\n","Epoch 12/15\n","123603/123603 [==============================] - 172s 1ms/sample - loss: 5.1871 - accuracy: 0.0064 - val_loss: 5.1867 - val_accuracy: 0.0067\n","Epoch 13/15\n","123603/123603 [==============================] - 172s 1ms/sample - loss: 5.1869 - accuracy: 0.0061 - val_loss: 5.1877 - val_accuracy: 0.0066\n","Epoch 14/15\n","123603/123603 [==============================] - 172s 1ms/sample - loss: 5.1871 - accuracy: 0.0063 - val_loss: 5.1859 - val_accuracy: 0.0077\n","Epoch 15/15\n","123603/123603 [==============================] - 172s 1ms/sample - loss: 5.1870 - accuracy: 0.0066 - val_loss: 5.1872 - val_accuracy: 0.0077\n","Train on 123603 samples, validate on 13734 samples\n","Epoch 16/20\n","123603/123603 [==============================] - 171s 1ms/sample - loss: 5.1872 - accuracy: 0.0067 - val_loss: 5.1868 - val_accuracy: 0.0077\n","Epoch 17/20\n","123603/123603 [==============================] - 171s 1ms/sample - loss: 5.1871 - accuracy: 0.0066 - val_loss: 5.1880 - val_accuracy: 0.0077\n","Epoch 18/20\n","123603/123603 [==============================] - 171s 1ms/sample - loss: 5.1870 - accuracy: 0.0066 - val_loss: 5.1873 - val_accuracy: 0.0058\n","Epoch 19/20\n","123603/123603 [==============================] - 171s 1ms/sample - loss: 5.1870 - accuracy: 0.0059 - val_loss: 5.1879 - val_accuracy: 0.0064\n","Epoch 20/20\n","123603/123603 [==============================] - 170s 1ms/sample - loss: 5.1869 - accuracy: 0.0060 - val_loss: 5.1881 - val_accuracy: 0.0077\n","Train on 123603 samples, validate on 13734 samples\n","Epoch 21/25\n","123603/123603 [==============================] - 170s 1ms/sample - loss: 5.1868 - accuracy: 0.0065 - val_loss: 5.1867 - val_accuracy: 0.0077\n","Epoch 22/25\n","123603/123603 [==============================] - 171s 1ms/sample - loss: 5.1869 - accuracy: 0.0061 - val_loss: 5.1872 - val_accuracy: 0.0063\n","Epoch 23/25\n","123603/123603 [==============================] - 170s 1ms/sample - loss: 5.1868 - accuracy: 0.0064 - val_loss: 5.1874 - val_accuracy: 0.0063\n","Epoch 24/25\n","123603/123603 [==============================] - 170s 1ms/sample - loss: 5.1869 - accuracy: 0.0062 - val_loss: 5.1869 - val_accuracy: 0.0077\n","Epoch 25/25\n","123603/123603 [==============================] - 170s 1ms/sample - loss: 5.1868 - accuracy: 0.0064 - val_loss: 5.1878 - val_accuracy: 0.0077\n","Train on 123603 samples, validate on 13734 samples\n","Epoch 26/30\n","123603/123603 [==============================] - 170s 1ms/sample - loss: 5.1867 - accuracy: 0.0065 - val_loss: 5.1867 - val_accuracy: 0.0066\n","Epoch 27/30\n","123603/123603 [==============================] - 170s 1ms/sample - loss: 5.1869 - accuracy: 0.0059 - val_loss: 5.1865 - val_accuracy: 0.0077\n","Epoch 28/30\n","123603/123603 [==============================] - 170s 1ms/sample - loss: 5.1868 - accuracy: 0.0063 - val_loss: 5.1867 - val_accuracy: 0.0066\n","Epoch 29/30\n","123603/123603 [==============================] - 170s 1ms/sample - loss: 5.1866 - accuracy: 0.0065 - val_loss: 5.1870 - val_accuracy: 0.0063\n","Epoch 30/30\n","123603/123603 [==============================] - 170s 1ms/sample - loss: 5.1869 - accuracy: 0.0062 - val_loss: 5.1863 - val_accuracy: 0.0063\n"]}],"source":["index= 0\n","step = 5\n","stop = 30\n","\n","while index < stop:\n","    model.fit(x_train, y_train, batch_size=batch, initial_epoch=index, epochs=index+step, validation_data=(x_val, y_val), callbacks=[tbCallBack])\n","    index += step\n","    model.save_weights('data/models/model_coarse'+str(index))\n","save_index = index"]},{"cell_type":"markdown","metadata":{"id":"soN_UzqRcEbY"},"source":["### Load Most Recent Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"spTxqr6TcEbY"},"outputs":[],"source":["sgd_fine = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"b9RmSvVJcEbY"},"outputs":[],"source":["for i in range(len(model.layers)):\n","    model.layers[i].trainable=False"]},{"cell_type":"markdown","metadata":{"id":"DNGkbfHacEbY"},"source":["## Fine-Tuning for Coarse Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Zbx-lrtncEbY"},"outputs":[],"source":["y_train_c = np.dot(y_train,fine2coarse)\n","y_val_c = np.dot(y_val,fine2coarse)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"nySjml_fcEbY"},"outputs":[],"source":["net = Conv2D(1024, 1, strides=1, padding='same', activation='elu')(model.layers[-8].output)\n","net = Conv2D(1152, 2, strides=1, padding='same', activation='elu')(net)\n","net = Dropout(.6)(net)\n","net = MaxPooling2D((2, 2), padding='same')(net)\n","\n","net = Flatten()(net)\n","net = Dense(1152, activation='elu')(net)\n","#out_coarse = Dense(20, activation='softmax')(net)\n","out_coarse = Dense(8, activation='softmax')(net)\n","\n","model_c = Model(inputs=in_layer,outputs=out_coarse)\n","model_c.compile(optimizer= sgd_coarse, loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","for i in range(len(model_c.layers)-1):\n","    model_c.layers[i].set_weights(model.layers[i].get_weights())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"QpegYWtTcEbY","outputId":"7d3394d5-3db8-40c0-92af-51398b8c9eca"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train on 123603 samples, validate on 13734 samples\n","Epoch 31/40\n","123603/123603 [==============================] - 52s 418us/sample - loss: 2.0433 - accuracy: 0.2067 - val_loss: 2.0380 - val_accuracy: 0.2103\n","Epoch 32/40\n","123603/123603 [==============================] - 52s 421us/sample - loss: 2.0424 - accuracy: 0.2069 - val_loss: 2.0415 - val_accuracy: 0.2103\n","Epoch 33/40\n","123603/123603 [==============================] - 52s 419us/sample - loss: 2.0423 - accuracy: 0.2069 - val_loss: 2.0390 - val_accuracy: 0.2103\n","Epoch 34/40\n","123603/123603 [==============================] - 52s 418us/sample - loss: 2.0420 - accuracy: 0.2069 - val_loss: 2.0406 - val_accuracy: 0.2103\n","Epoch 35/40\n","123603/123603 [==============================] - 52s 420us/sample - loss: 2.0417 - accuracy: 0.2069 - val_loss: 2.0395 - val_accuracy: 0.2103\n","Epoch 36/40\n","123603/123603 [==============================] - 52s 422us/sample - loss: 2.0416 - accuracy: 0.2069 - val_loss: 2.0391 - val_accuracy: 0.2103\n","Epoch 37/40\n","123603/123603 [==============================] - 52s 422us/sample - loss: 2.0415 - accuracy: 0.2069 - val_loss: 2.0394 - val_accuracy: 0.2103\n","Epoch 38/40\n","123603/123603 [==============================] - 52s 420us/sample - loss: 2.0415 - accuracy: 0.2069 - val_loss: 2.0390 - val_accuracy: 0.2103\n","Epoch 39/40\n","123603/123603 [==============================] - 52s 420us/sample - loss: 2.0414 - accuracy: 0.2069 - val_loss: 2.0387 - val_accuracy: 0.2103\n","Epoch 40/40\n","123603/123603 [==============================] - 52s 420us/sample - loss: 2.0411 - accuracy: 0.2069 - val_loss: 2.0377 - val_accuracy: 0.2103\n"]}],"source":["index = 30\n","step = 10\n","stop = 40\n","\n","while index < stop:\n","    model_c.fit(x_train, y_train_c, batch_size=batch, initial_epoch=index, epochs=index+step, validation_data=(x_val, y_val_c), callbacks=[tbCallBack])\n","    index += step"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5PgpL0DlcEbZ","outputId":"183bf36d-f2b9-4f3e-aefc-8757308622ea"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train on 123603 samples, validate on 13734 samples\n","Epoch 41/50\n","123603/123603 [==============================] - 52s 423us/sample - loss: 2.0404 - accuracy: 0.2069 - val_loss: 2.0382 - val_accuracy: 0.2103\n","Epoch 42/50\n","123603/123603 [==============================] - 52s 421us/sample - loss: 2.0403 - accuracy: 0.2069 - val_loss: 2.0379 - val_accuracy: 0.2103\n","Epoch 43/50\n","123603/123603 [==============================] - 52s 418us/sample - loss: 2.0403 - accuracy: 0.2069 - val_loss: 2.0379 - val_accuracy: 0.2103\n","Epoch 44/50\n","123603/123603 [==============================] - 52s 419us/sample - loss: 2.0403 - accuracy: 0.2069 - val_loss: 2.0379 - val_accuracy: 0.2103\n","Epoch 45/50\n","123603/123603 [==============================] - 52s 419us/sample - loss: 2.0403 - accuracy: 0.2069 - val_loss: 2.0380 - val_accuracy: 0.2103\n","Epoch 46/50\n","123603/123603 [==============================] - 52s 419us/sample - loss: 2.0404 - accuracy: 0.2069 - val_loss: 2.0380 - val_accuracy: 0.2103\n","Epoch 47/50\n","123603/123603 [==============================] - 51s 415us/sample - loss: 2.0403 - accuracy: 0.2069 - val_loss: 2.0381 - val_accuracy: 0.2103\n","Epoch 48/50\n","123603/123603 [==============================] - 52s 417us/sample - loss: 2.0403 - accuracy: 0.2069 - val_loss: 2.0382 - val_accuracy: 0.2103\n","Epoch 49/50\n","123603/123603 [==============================] - 52s 418us/sample - loss: 2.0403 - accuracy: 0.2069 - val_loss: 2.0378 - val_accuracy: 0.2103\n","Epoch 50/50\n","123603/123603 [==============================] - 52s 422us/sample - loss: 2.0403 - accuracy: 0.2069 - val_loss: 2.0377 - val_accuracy: 0.2103\n"]}],"source":["model_c.compile(optimizer=sgd_fine, loss='categorical_crossentropy', metrics=['accuracy'])\n","stop = 50\n","\n","while index < stop:\n","    model_c.fit(x_train, y_train_c, batch_size=batch, initial_epoch=index, epochs=index+step, validation_data=(x_val, y_val_c), callbacks=[tbCallBack])\n","    index += step"]},{"cell_type":"markdown","metadata":{"id":"tYaJC0GGcEbZ"},"source":["## Fine-Tuning for Fine Classifiers"]},{"cell_type":"markdown","metadata":{"id":"hsgXHfTGcEbZ"},"source":["### Construct Fine Classifiers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LdlSXgHxcEbZ"},"outputs":[],"source":["def fine_model():\n","    net = Conv2D(1024, 1, strides=1, padding='same', activation='elu')(model.layers[-8].output)\n","    net = Conv2D(1152, 2, strides=1, padding='same', activation='elu')(net)\n","    net = Dropout(.6)(net)\n","    net = MaxPooling2D((2, 2), padding='same')(net)\n","\n","    net = Flatten()(net)\n","    net = Dense(1152, activation='elu')(net)\n","#    out_fine = Dense(100, activation='softmax')(net)\n","    out_fine = Dense(180, activation='softmax')(net)\n","    model_fine = Model(inputs=in_layer,outputs=out_fine)\n","    model_fine.compile(optimizer= sgd_coarse,\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","    for i in range(len(model_fine.layers)-1):\n","        model_fine.layers[i].set_weights(model.layers[i].get_weights())\n","    return model_fine"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Y6w0IdI2cEbZ"},"outputs":[],"source":["fine_models = {'models' : [{} for i in range(coarse_categories)], 'yhf' : [{} for i in range(coarse_categories)]}\n","for i in range(coarse_categories):\n","    model_i = fine_model()\n","    fine_models['models'][i] = model_i"]},{"cell_type":"markdown","metadata":{"id":"6MOn-yjDcEbZ"},"source":["### Train Fine Classifiers on Respective Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"L9GX9_e_cEbZ"},"outputs":[],"source":["def get_error(y,yh):\n","    # Threshold\n","    yht = np.zeros(np.shape(yh))\n","    yht[np.arange(len(yh)), yh.argmax(1)] = 1\n","    # Evaluate Error\n","    error = np.count_nonzero(np.count_nonzero(y-yht,1))/len(y)\n","    return error"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"uIkQdB3UcEbZ","outputId":"bb6d7735-05ab-45d3-89a6-19afd7d12aff"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train on 25579 samples, validate on 2888 samples\n","Epoch 1/5\n","25579/25579 [==============================] - 13s 496us/sample - loss: 3.8498 - accuracy: 0.0285 - val_loss: 3.6611 - val_accuracy: 0.0305\n","Epoch 2/5\n","25579/25579 [==============================] - 11s 428us/sample - loss: 3.6527 - accuracy: 0.0286 - val_loss: 3.6396 - val_accuracy: 0.0308\n","Epoch 3/5\n","25579/25579 [==============================] - 11s 429us/sample - loss: 3.6445 - accuracy: 0.0281 - val_loss: 3.6348 - val_accuracy: 0.0308\n","Epoch 4/5\n","25579/25579 [==============================] - 11s 430us/sample - loss: 3.6388 - accuracy: 0.0299 - val_loss: 3.6275 - val_accuracy: 0.0305\n","Epoch 5/5\n","25579/25579 [==============================] - 11s 424us/sample - loss: 3.6344 - accuracy: 0.0270 - val_loss: 3.6321 - val_accuracy: 0.0229\n","Train on 25579 samples, validate on 2888 samples\n","Epoch 6/10\n","25579/25579 [==============================] - 12s 454us/sample - loss: 3.6193 - accuracy: 0.0298 - val_loss: 3.6118 - val_accuracy: 0.0301\n","Epoch 7/10\n","25579/25579 [==============================] - 11s 432us/sample - loss: 3.6166 - accuracy: 0.0278 - val_loss: 3.6123 - val_accuracy: 0.0301\n","Epoch 8/10\n","25579/25579 [==============================] - 11s 426us/sample - loss: 3.6162 - accuracy: 0.0310 - val_loss: 3.6104 - val_accuracy: 0.0246\n","Epoch 9/10\n","25579/25579 [==============================] - 11s 430us/sample - loss: 3.6165 - accuracy: 0.0305 - val_loss: 3.6104 - val_accuracy: 0.0291\n","Epoch 10/10\n","25579/25579 [==============================] - 11s 432us/sample - loss: 3.6165 - accuracy: 0.0289 - val_loss: 3.6121 - val_accuracy: 0.0291\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n"]},{"name":"stdout","output_type":"stream","text":["Fine Classifier 0 Error: 0.9709141274238227\n","Train on 11911 samples, validate on 1259 samples\n","Epoch 1/5\n","11911/11911 [==============================] - 6s 526us/sample - loss: 3.2913 - accuracy: 0.0599 - val_loss: 2.8987 - val_accuracy: 0.0508\n","Epoch 2/5\n","11911/11911 [==============================] - 5s 443us/sample - loss: 2.8746 - accuracy: 0.0583 - val_loss: 2.8870 - val_accuracy: 0.0548\n","Epoch 3/5\n","11911/11911 [==============================] - 5s 430us/sample - loss: 2.8694 - accuracy: 0.0576 - val_loss: 2.8707 - val_accuracy: 0.0667\n","Epoch 4/5\n","11911/11911 [==============================] - 5s 427us/sample - loss: 2.8656 - accuracy: 0.0611 - val_loss: 2.8505 - val_accuracy: 0.0675\n","Epoch 5/5\n","11911/11911 [==============================] - 5s 433us/sample - loss: 2.8604 - accuracy: 0.0594 - val_loss: 2.8569 - val_accuracy: 0.0548\n","Train on 11911 samples, validate on 1259 samples\n","Epoch 6/10\n","11911/11911 [==============================] - 6s 485us/sample - loss: 2.8458 - accuracy: 0.0597 - val_loss: 2.8337 - val_accuracy: 0.0659\n","Epoch 7/10\n","11911/11911 [==============================] - 5s 425us/sample - loss: 2.8428 - accuracy: 0.0631 - val_loss: 2.8347 - val_accuracy: 0.0659\n","Epoch 8/10\n","11911/11911 [==============================] - 5s 436us/sample - loss: 2.8418 - accuracy: 0.0630 - val_loss: 2.8314 - val_accuracy: 0.0675\n","Epoch 9/10\n","11911/11911 [==============================] - 5s 439us/sample - loss: 2.8414 - accuracy: 0.0613 - val_loss: 2.8357 - val_accuracy: 0.0588\n","Epoch 10/10\n","11911/11911 [==============================] - 5s 435us/sample - loss: 2.8419 - accuracy: 0.0639 - val_loss: 2.8348 - val_accuracy: 0.0643\n","Fine Classifier 1 Error: 0.9356632247815727\n","Train on 10304 samples, validate on 1142 samples\n","Epoch 1/5\n","10304/10304 [==============================] - 6s 573us/sample - loss: 3.1932 - accuracy: 0.0713 - val_loss: 2.7340 - val_accuracy: 0.0736\n","Epoch 2/5\n","10304/10304 [==============================] - 4s 433us/sample - loss: 2.7428 - accuracy: 0.0712 - val_loss: 2.7314 - val_accuracy: 0.0569\n","Epoch 3/5\n","10304/10304 [==============================] - 4s 427us/sample - loss: 2.7371 - accuracy: 0.0651 - val_loss: 2.7224 - val_accuracy: 0.0648\n","Epoch 4/5\n","10304/10304 [==============================] - 4s 434us/sample - loss: 2.7337 - accuracy: 0.0722 - val_loss: 2.7458 - val_accuracy: 0.0736\n","Epoch 5/5\n","10304/10304 [==============================] - 4s 430us/sample - loss: 2.7319 - accuracy: 0.0726 - val_loss: 2.7160 - val_accuracy: 0.0648\n","Train on 10304 samples, validate on 1142 samples\n","Epoch 6/10\n","10304/10304 [==============================] - 5s 497us/sample - loss: 2.7161 - accuracy: 0.0720 - val_loss: 2.7083 - val_accuracy: 0.0771\n","Epoch 7/10\n","10304/10304 [==============================] - 4s 429us/sample - loss: 2.7149 - accuracy: 0.0730 - val_loss: 2.7077 - val_accuracy: 0.0823\n","Epoch 8/10\n","10304/10304 [==============================] - 4s 427us/sample - loss: 2.7142 - accuracy: 0.0679 - val_loss: 2.7076 - val_accuracy: 0.0823\n","Epoch 9/10\n","10304/10304 [==============================] - 4s 423us/sample - loss: 2.7141 - accuracy: 0.0695 - val_loss: 2.7070 - val_accuracy: 0.0823\n","Epoch 10/10\n","10304/10304 [==============================] - 4s 421us/sample - loss: 2.7132 - accuracy: 0.0759 - val_loss: 2.7078 - val_accuracy: 0.0771\n","Fine Classifier 2 Error: 0.9229422066549913\n","Train on 11303 samples, validate on 1289 samples\n","Epoch 1/5\n","11303/11303 [==============================] - 7s 596us/sample - loss: 3.2915 - accuracy: 0.0567 - val_loss: 2.8591 - val_accuracy: 0.0613\n","Epoch 2/5\n","11303/11303 [==============================] - 5s 421us/sample - loss: 2.8751 - accuracy: 0.0609 - val_loss: 2.8643 - val_accuracy: 0.0465\n","Epoch 3/5\n","11303/11303 [==============================] - 5s 428us/sample - loss: 2.8700 - accuracy: 0.0612 - val_loss: 2.8503 - val_accuracy: 0.0636\n","Epoch 4/5\n","11303/11303 [==============================] - 5s 435us/sample - loss: 2.8645 - accuracy: 0.0591 - val_loss: 2.8379 - val_accuracy: 0.0815\n","Epoch 5/5\n","11303/11303 [==============================] - 5s 426us/sample - loss: 2.8641 - accuracy: 0.0601 - val_loss: 2.8492 - val_accuracy: 0.0613\n","Train on 11303 samples, validate on 1289 samples\n","Epoch 6/10\n","11303/11303 [==============================] - 6s 502us/sample - loss: 2.8453 - accuracy: 0.0625 - val_loss: 2.8321 - val_accuracy: 0.0815\n","Epoch 7/10\n","11303/11303 [==============================] - 5s 428us/sample - loss: 2.8438 - accuracy: 0.0622 - val_loss: 2.8308 - val_accuracy: 0.0566\n","Epoch 8/10\n","11303/11303 [==============================] - 5s 435us/sample - loss: 2.8450 - accuracy: 0.0601 - val_loss: 2.8344 - val_accuracy: 0.0815\n","Epoch 9/10\n","11303/11303 [==============================] - 5s 420us/sample - loss: 2.8426 - accuracy: 0.0623 - val_loss: 2.8327 - val_accuracy: 0.0636\n","Epoch 10/10\n","11303/11303 [==============================] - 5s 428us/sample - loss: 2.8416 - accuracy: 0.0628 - val_loss: 2.8353 - val_accuracy: 0.0636\n","Fine Classifier 3 Error: 0.9363847944142746\n","Train on 15610 samples, validate on 1806 samples\n","Epoch 1/5\n","15610/15610 [==============================] - 9s 546us/sample - loss: 3.4573 - accuracy: 0.0505 - val_loss: 3.1316 - val_accuracy: 0.0493\n","Epoch 2/5\n","15610/15610 [==============================] - 7s 430us/sample - loss: 3.1277 - accuracy: 0.0484 - val_loss: 3.1226 - val_accuracy: 0.0520\n","Epoch 3/5\n","15610/15610 [==============================] - 7s 426us/sample - loss: 3.1229 - accuracy: 0.0502 - val_loss: 3.1053 - val_accuracy: 0.0493\n","Epoch 4/5\n","15610/15610 [==============================] - 7s 424us/sample - loss: 3.1187 - accuracy: 0.0495 - val_loss: 3.1070 - val_accuracy: 0.0515\n","Epoch 5/5\n","15610/15610 [==============================] - 7s 438us/sample - loss: 3.1166 - accuracy: 0.0471 - val_loss: 3.1094 - val_accuracy: 0.0432\n","Train on 15610 samples, validate on 1806 samples\n","Epoch 6/10\n","15610/15610 [==============================] - 7s 470us/sample - loss: 3.1000 - accuracy: 0.0507 - val_loss: 3.0915 - val_accuracy: 0.0449\n","Epoch 7/10\n","15610/15610 [==============================] - 7s 433us/sample - loss: 3.0985 - accuracy: 0.0509 - val_loss: 3.0925 - val_accuracy: 0.0449\n","Epoch 8/10\n","15610/15610 [==============================] - 7s 423us/sample - loss: 3.0994 - accuracy: 0.0508 - val_loss: 3.0901 - val_accuracy: 0.0449\n","Epoch 9/10\n","15610/15610 [==============================] - 7s 431us/sample - loss: 3.0969 - accuracy: 0.0516 - val_loss: 3.0877 - val_accuracy: 0.0587\n","Epoch 10/10\n","15610/15610 [==============================] - 7s 423us/sample - loss: 3.0969 - accuracy: 0.0526 - val_loss: 3.0888 - val_accuracy: 0.0515\n","Fine Classifier 4 Error: 0.9485049833887044\n","Train on 15864 samples, validate on 1677 samples\n","Epoch 1/5\n","15864/15864 [==============================] - 9s 541us/sample - loss: 3.5018 - accuracy: 0.0420 - val_loss: 3.1769 - val_accuracy: 0.0429\n","Epoch 2/5\n","15864/15864 [==============================] - 7s 419us/sample - loss: 3.1783 - accuracy: 0.0468 - val_loss: 3.1756 - val_accuracy: 0.0429\n","Epoch 3/5\n","15864/15864 [==============================] - 7s 422us/sample - loss: 3.1735 - accuracy: 0.0432 - val_loss: 3.1700 - val_accuracy: 0.0417\n","Epoch 4/5\n","15864/15864 [==============================] - 7s 421us/sample - loss: 3.1702 - accuracy: 0.0419 - val_loss: 3.1661 - val_accuracy: 0.0405\n","Epoch 5/5\n","15864/15864 [==============================] - 7s 436us/sample - loss: 3.1688 - accuracy: 0.0426 - val_loss: 3.1607 - val_accuracy: 0.0429\n","Train on 15864 samples, validate on 1677 samples\n","Epoch 6/10\n","15864/15864 [==============================] - 7s 471us/sample - loss: 3.1497 - accuracy: 0.0449 - val_loss: 3.1414 - val_accuracy: 0.0471\n","Epoch 7/10\n","15864/15864 [==============================] - 7s 423us/sample - loss: 3.1474 - accuracy: 0.0442 - val_loss: 3.1428 - val_accuracy: 0.0417\n","Epoch 8/10\n","15864/15864 [==============================] - 7s 414us/sample - loss: 3.1470 - accuracy: 0.0458 - val_loss: 3.1437 - val_accuracy: 0.0405\n","Epoch 9/10\n","15864/15864 [==============================] - 7s 432us/sample - loss: 3.1451 - accuracy: 0.0448 - val_loss: 3.1396 - val_accuracy: 0.0453\n","Epoch 10/10\n","15864/15864 [==============================] - 7s 416us/sample - loss: 3.1457 - accuracy: 0.0419 - val_loss: 3.1407 - val_accuracy: 0.0471\n","Fine Classifier 5 Error: 0.9528920691711389\n","Train on 16693 samples, validate on 1892 samples\n","Epoch 1/5\n","16693/16693 [==============================] - 9s 554us/sample - loss: 3.5871 - accuracy: 0.0431 - val_loss: 3.2805 - val_accuracy: 0.0476\n","Epoch 2/5\n","16693/16693 [==============================] - 7s 417us/sample - loss: 3.2786 - accuracy: 0.0416 - val_loss: 3.2706 - val_accuracy: 0.0370\n","Epoch 3/5\n","16693/16693 [==============================] - 7s 428us/sample - loss: 3.2734 - accuracy: 0.0436 - val_loss: 3.2606 - val_accuracy: 0.0476\n","Epoch 4/5\n","16693/16693 [==============================] - 7s 416us/sample - loss: 3.2655 - accuracy: 0.0456 - val_loss: 3.2770 - val_accuracy: 0.0375\n","Epoch 5/5\n","16693/16693 [==============================] - 7s 426us/sample - loss: 3.2642 - accuracy: 0.0428 - val_loss: 3.2623 - val_accuracy: 0.0544\n","Train on 16693 samples, validate on 1892 samples\n","Epoch 6/10\n","16693/16693 [==============================] - 8s 470us/sample - loss: 3.2481 - accuracy: 0.0443 - val_loss: 3.2426 - val_accuracy: 0.0465\n","Epoch 7/10\n","16693/16693 [==============================] - 7s 422us/sample - loss: 3.2444 - accuracy: 0.0456 - val_loss: 3.2433 - val_accuracy: 0.0375\n","Epoch 8/10\n","16693/16693 [==============================] - 7s 427us/sample - loss: 3.2450 - accuracy: 0.0458 - val_loss: 3.2418 - val_accuracy: 0.0375\n","Epoch 9/10\n","16693/16693 [==============================] - 7s 429us/sample - loss: 3.2450 - accuracy: 0.0448 - val_loss: 3.2413 - val_accuracy: 0.0375\n","Epoch 10/10\n","16693/16693 [==============================] - 7s 429us/sample - loss: 3.2435 - accuracy: 0.0449 - val_loss: 3.2411 - val_accuracy: 0.0476\n","Fine Classifier 6 Error: 0.952431289640592\n","Train on 16339 samples, validate on 1781 samples\n","Epoch 1/5\n","16339/16339 [==============================] - 8s 485us/sample - loss: 3.4636 - accuracy: 0.0463 - val_loss: 3.1645 - val_accuracy: 0.0365\n","Epoch 2/5\n","16339/16339 [==============================] - 7s 422us/sample - loss: 3.1656 - accuracy: 0.0435 - val_loss: 3.1507 - val_accuracy: 0.0354\n","Epoch 3/5\n","16339/16339 [==============================] - 7s 423us/sample - loss: 3.1566 - accuracy: 0.0433 - val_loss: 3.1668 - val_accuracy: 0.0354\n","Epoch 4/5\n","16339/16339 [==============================] - 7s 429us/sample - loss: 3.1530 - accuracy: 0.0470 - val_loss: 3.1368 - val_accuracy: 0.0590\n","Epoch 5/5\n","16339/16339 [==============================] - 7s 425us/sample - loss: 3.1496 - accuracy: 0.0477 - val_loss: 3.1587 - val_accuracy: 0.0477\n","Train on 16339 samples, validate on 1781 samples\n","Epoch 6/10\n","16339/16339 [==============================] - 8s 495us/sample - loss: 3.1332 - accuracy: 0.0489 - val_loss: 3.1325 - val_accuracy: 0.0511\n","Epoch 7/10\n","16339/16339 [==============================] - 7s 421us/sample - loss: 3.1311 - accuracy: 0.0479 - val_loss: 3.1339 - val_accuracy: 0.0376\n","Epoch 8/10\n","16339/16339 [==============================] - 7s 421us/sample - loss: 3.1293 - accuracy: 0.0488 - val_loss: 3.1304 - val_accuracy: 0.0511\n","Epoch 9/10\n","16339/16339 [==============================] - 7s 420us/sample - loss: 3.1285 - accuracy: 0.0466 - val_loss: 3.1318 - val_accuracy: 0.0590\n","Epoch 10/10\n","16339/16339 [==============================] - 7s 416us/sample - loss: 3.1319 - accuracy: 0.0477 - val_loss: 3.1337 - val_accuracy: 0.0354\n","Fine Classifier 7 Error: 0.9646266142616507\n"]}],"source":["for i in range(coarse_categories):\n","    index= 0\n","    step = 5\n","    stop = 5\n","\n","    # Get all training data for the coarse category\n","    ix = np.where([(y_train[:,j]==1) for j in [k for k, e in enumerate(fine2coarse[:,i]) if e != 0]])[1]\n","    x_tix = x_train[ix]\n","    y_tix = y_train[ix]\n","\n","    # Get all validation data for the coarse category\n","    ix_v = np.where([(y_val[:,j]==1) for j in [k for k, e in enumerate(fine2coarse[:,i]) if e != 0]])[1]\n","    x_vix = x_val[ix_v]\n","    y_vix = y_val[ix_v]\n","\n","    while index < stop:\n","        fine_models['models'][i].fit(x_tix, y_tix, batch_size=batch, initial_epoch=index, epochs=index+step, validation_data=(x_vix, y_vix))\n","        index += step\n","\n","    fine_models['models'][i].compile(optimizer=sgd_fine, loss='categorical_crossentropy', metrics=['accuracy'])\n","    stop = 10\n","\n","    while index < stop:\n","        fine_models['models'][i].fit(x_tix, y_tix, batch_size=batch, initial_epoch=index, epochs=index+step, validation_data=(x_vix, y_vix))\n","        index += step\n","\n","    yh_f = fine_models['models'][i].predict(x_val[ix_v], batch_size=batch)\n","    print('Fine Classifier '+str(i)+' Error: '+str(get_error(y_val[ix_v],yh_f)))"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"VbHkb_q9cEbZ"},"source":["## Probabilistic Averaging"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Mcpm_8MHcEba"},"outputs":[],"source":["def eval_hdcnn(X, y):\n","    yh = np.zeros(np.shape(y))\n","\n","    yh_s = model.predict(X, batch_size=batch)\n","\n","    print('Single Classifier Error: '+str(get_error(y,yh_s)))\n","\n","    yh_c = model_c.predict(X, batch_size=batch)\n","    y_c = np.dot(y,fine2coarse)\n","\n","    print('Coarse Classifier Error: '+str(get_error(y_c,yh_c)))\n","\n","    for i in range(coarse_categories):\n","        if i%5 == 0:\n","            print(\"Evaluating Fine Classifier: \", str(i))\n","        fine_models['yhf'][i] = fine_models['models'][i].predict(X, batch_size=batch)\n","        yh += np.multiply(yh_c[:,i].reshape((len(y)),1), fine_models['yhf'][i])\n","\n","    print('Overall Error: '+str(get_error(y,yh)))\n","    return yh"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"EcxdW1ZDcEba","outputId":"d858191e-b761-4385-800a-b3536d3e251b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Single Classifier Error: 0.9936653560506772\n","Coarse Classifier Error: 0.7897189456822484\n","Evaluating Fine Classifier:  0\n","Evaluating Fine Classifier:  5\n","Overall Error: 0.9954128440366973\n"]}],"source":["yh = eval_hdcnn(x_val,y_val)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"toc_visible":true,"gpuClass":"premium"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"toc":{"colors":{"hover_highlight":"#DAA520","running_highlight":"#FF0000","selected_highlight":"#FFD700"},"moveMenuLeft":true,"nav_menu":{"height":"12px","width":"252px"},"navigate_menu":true,"number_sections":true,"sideBar":true,"threshold":4,"toc_cell":false,"toc_section_display":"block","toc_window_display":false,"widenNotebook":false}},"nbformat":4,"nbformat_minor":0}